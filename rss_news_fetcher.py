#!/usr/bin/env python3
"""
RSS News Fetcher
Fetches news articles from major RSS feeds with image extraction
"""
import json
import datetime
import requests
import feedparser
from urllib.parse import urlparse
import os
from bs4 import BeautifulSoup
import time
import concurrent.futures
from threading import Lock

class RSSNewsFetcher:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.lock = Lock()
        
        # RSS feeds organized by category - EXACTLY as you specified
        self.rss_feeds = {
            'international': {
                'BBC News': 'http://feeds.bbci.co.uk/news/rss.xml',
                'CNN': 'http://rss.cnn.com/rss/edition.rss',
                'Reuters': 'https://www.reuters.com/tools/rss',
                'AP News': 'https://apnews.com/apf-topnews',
                'The Guardian': 'https://www.theguardian.com/world/rss'
            },
            'us_news': {
                'NPR': 'https://feeds.npr.org/1001/rss.xml',
                'Fox News': 'https://moxie.foxnews.com/google-publisher/latest.xml',
                'CBS News': 'https://www.cbsnews.com/latest/rss/main',
                'ABC News': 'https://abcnews.go.com/abcnews/topstories'
            },
            'technology': {
                'TechCrunch': 'https://techcrunch.com/feed/',
                'The Verge': 'https://www.theverge.com/rss/index.xml',
                'Ars Technica': 'https://feeds.arstechnica.com/arstechnica/index',
                'Wired': 'https://www.wired.com/feed/rss'
            },
            'business': {
                'Financial Times': 'https://www.ft.com/rss/home',
                'Wall Street Journal': 'https://feeds.a.dj.com/rss/RSSWorldNews.xml',
                'Bloomberg': 'https://feeds.bloomberg.com/markets/news.rss',
                'Forbes': 'https://www.forbes.com/real-time/feed2/'
            },
            'sports': {
                'ESPN': 'https://www.espn.com/espn/rss/news',
                'BBC Sport': 'http://feeds.bbci.co.uk/sport/rss.xml',
                'Sky Sports': 'https://www.skysports.com/rss/12040'
            }
        }

    def extract_image_from_article(self, article_url, timeout=10):
        """Extract image URL from article content"""
        try:
            response = self.session.get(article_url, timeout=timeout)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Try multiple methods to find images
                image_selectors = [
                    ('meta', {'property': 'og:image'}),
                    ('meta', {'name': 'twitter:image'}),
                    ('meta', {'property': 'twitter:image'}),
                    ('meta', {'name': 'og:image'}),
                    ('img', {'class': lambda x: x and any(term in x.lower() for term in ['hero', 'featured', 'main', 'article'])})
                ]
                
                for tag, attrs in image_selectors:
                    element = soup.find(tag, attrs)
                    if element:
                        if tag == 'meta':
                            image_url = element.get('content')
                        else:
                            image_url = element.get('src') or element.get('data-src')
                        
                        if image_url:
                            # Make relative URLs absolute
                            if image_url.startswith('//'):
                                image_url = 'https:' + image_url
                            elif image_url.startswith('/'):
                                parsed_url = urlparse(article_url)
                                image_url = f"{parsed_url.scheme}://{parsed_url.netloc}{image_url}"
                            
                            # Validate image URL
                            if image_url.startswith('http') and any(ext in image_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif']):
                                return image_url
                            
        except Exception as e:
            print(f"Error extracting image from {article_url}: {e}")
        
        return None

    def extract_image_from_feed_entry(self, entry):
        """Extract image from RSS feed entry itself"""
        # Check for media content in feed
        if hasattr(entry, 'media_content') and entry.media_content:
            for media in entry.media_content:
                if media.get('type', '').startswith('image/'):
                    return media.get('url')
        
        # Check for enclosures
        if hasattr(entry, 'enclosures') and entry.enclosures:
            for enclosure in entry.enclosures:
                if enclosure.get('type', '').startswith('image/'):
                    return enclosure.get('href')
        
        # Check for media thumbnail
        if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
            return entry.media_thumbnail[0].get('url')
        
        # Check summary/description for images
        content = entry.get('summary', '') or entry.get('description', '')
        if content:
            soup = BeautifulSoup(content, 'html.parser')
            img = soup.find('img')
            if img:
                return img.get('src')
        
        return None

    def process_feed(self, source_name, feed_url, category):
        """Process a single RSS feed"""
        articles = []
        try:
            print(f"Fetching {source_name} ({category})...")
            
            # Parse RSS feed
            feed = feedparser.parse(feed_url)
            
            if feed.bozo and feed.bozo_exception:
                print(f"Warning: Feed parsing issue for {source_name}: {feed.bozo_exception}")
            
            for entry in feed.entries[:20]:  # Limit to 20 articles per source
                # Extract basic article info
                article = {
                    'title': entry.get('title', '').strip(),
                    'link': entry.get('link', ''),
                    'published': entry.get('published', ''),
                    'summary': entry.get('summary', '').strip(),
                    'description': entry.get('description', '').strip(),
                    'source': source_name,
                    'category': category,
                    'rss_url': feed_url,
                    'image_url': '',
                    'has_image': False,
                    'author': entry.get('author', ''),
                    'tags': [tag.term for tag in entry.get('tags', [])]
                }
                
                # Skip articles without title or link
                if not article['title'] or not article['link']:
                    continue
                
                # Try to extract image from feed entry first
                image_url = self.extract_image_from_feed_entry(entry)
                
                # If no image in feed, try to extract from article page
                if not image_url and article['link']:
                    image_url = self.extract_image_from_article(article['link'])
                
                if image_url:
                    article['image_url'] = image_url
                    article['has_image'] = True
                
                articles.append(article)
                
        except Exception as e:
            print(f"Error processing feed {source_name}: {e}")
        
        return articles

    def fetch_all_news(self, max_workers=5):
        """Fetch news from all RSS feeds"""
        print("🚀 Starting RSS news extraction...")
        print(f"📡 Processing {sum(len(feeds) for feeds in self.rss_feeds.values())} RSS feeds...")
        
        news_data = {
            'extraction_timestamp': datetime.datetime.now().isoformat(),
            'total_articles': 0,
            'articles_with_images': 0,
            'image_success_rate': '0%',
            'sources_processed': 0,
            'categories': list(self.rss_feeds.keys()),
            'by_category': {},
            'by_source': {},
            'feed_status': {}
        }
        
        # Initialize category data
        for category in self.rss_feeds.keys():
            news_data['by_category'][category] = []
        
        all_tasks = []
        
        # Prepare all feed processing tasks
        for category, feeds in self.rss_feeds.items():
            for source_name, feed_url in feeds.items():
                all_tasks.append((source_name, feed_url, category))
        
        # Process feeds concurrently
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_task = {
                executor.submit(self.process_feed, source_name, feed_url, category): (source_name, category)
                for source_name, feed_url, category in all_tasks
            }
            
            for future in concurrent.futures.as_completed(future_to_task):
                source_name, category = future_to_task[future]
                try:
                    articles = future.result()
                    
                    # Add articles to category
                    news_data['by_category'][category].extend(articles)
                    
                    # Add articles to source
                    news_data['by_source'][source_name] = articles
                    
                    # Track feed status
                    news_data['feed_status'][source_name] = {
                        'status': 'success',
                        'articles_count': len(articles),
                        'category': category
                    }
                    
                    print(f"✅ {source_name}: {len(articles)} articles")
                    
                except Exception as e:
                    print(f"❌ {source_name}: Failed - {e}")
                    news_data['feed_status'][source_name] = {
                        'status': 'failed',
                        'error': str(e),
                        'category': category
                    }
        
        # Calculate statistics
        all_articles = []
        for category, articles in news_data['by_category'].items():
            all_articles.extend(articles)
        
        news_data['total_articles'] = len(all_articles)
        news_data['articles_with_images'] = sum(1 for article in all_articles if article['has_image'])
        news_data['sources_processed'] = len([status for status in news_data['feed_status'].values() if status['status'] == 'success'])
        
        if news_data['total_articles'] > 0:
            image_success_rate = (news_data['articles_with_images'] / news_data['total_articles']) * 100
            news_data['image_success_rate'] = f"{image_success_rate:.1f}%"
        
        return news_data

    def save_to_json(self, news_data, filename='rss_news_data.json'):
        """Save news data to JSON file"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(news_data, f, indent=2, ensure_ascii=False)
        print(f"💾 News data saved to {filename}")

def main():
    fetcher = RSSNewsFetcher()
    
    # Fetch news from all RSS feeds
    news_data = fetcher.fetch_all_news()
    
    # Print summary
    print("\n" + "="*60)
    print("📊 RSS NEWS EXTRACTION SUMMARY")
    print("="*60)
    print(f"📰 Total articles extracted: {news_data['total_articles']}")
    print(f"🖼️  Articles with images: {news_data['articles_with_images']} ({news_data['image_success_rate']})")
    print(f"✅ Sources processed successfully: {news_data['sources_processed']}")
    print(f"📂 Categories: {', '.join(news_data['categories'])}")
    
    print("\n📊 By Category:")
    for category, articles in news_data['by_category'].items():
        print(f"  📁 {category.replace('_', ' ').title()}: {len(articles)} articles")
    
    print("\n🔍 Feed Status:")
    for source, status in news_data['feed_status'].items():
        status_icon = "✅" if status['status'] == 'success' else "❌"
        if status['status'] == 'success':
            print(f"  {status_icon} {source}: {status['articles_count']} articles")
        else:
            print(f"  {status_icon} {source}: {status.get('error', 'Unknown error')}")
    
    # Save to JSON file
    fetcher.save_to_json(news_data)
    
    print(f"\n🎉 Complete! Your news data is saved in 'rss_news_data.json'")
    print(f"📁 File size: {os.path.getsize('rss_news_data.json') / 1024:.1f} KB")
    
    return news_data

if __name__ == "__main__":
    main()